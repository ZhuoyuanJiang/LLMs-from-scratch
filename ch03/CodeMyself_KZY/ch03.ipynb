{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# Check PyTorch version \n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "\n",
    "# Check if CUDA is available and which version\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "print(f'CUDA version: {torch.version.cuda if torch.cuda.is_available() else \"Not available\"}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version \n",
    "print(f'torch version: {version(\"torch\")}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "\n",
    "print(inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Step 1 calculate attention score \n",
    "\n",
    "query = inputs[1] # 2nd input token is the query \n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs): # enumerate iterates over the first dimension \n",
    "    # print(f'x_i = {x_i}')\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "print(attn_scores_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# Normalize attention scores \n",
    "\n",
    "attn_weights_2_tmp = attn_scores_2/attn_scores_2.sum()\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Calculate attention weights using softmax \n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0) # because we are going to softmax a 1D tensor, the dimension doesn't matter \n",
    "    # Usually, for 2D tensor, we use dim=1.\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# In practice we use pytorch's softmax function \n",
    "\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context vector: tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Calculate context vector \n",
    "\n",
    "query = inputs[1] # 2nd input token is the query \n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    # print(f'x_i = {x_i} and attn_weights_i = {attn_weights_2[i]}')\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(\"Context vector:\", context_vec_2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate Context vector for all query tokens \n",
    "\n",
    "# attention scores for all query tokens \n",
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i,j] = torch.dot(x_i, x_j) # attn_score should be a symmetric matrix\n",
    "print(attn_scores)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n",
      "tensor shape = torch.Size([6, 6])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = input @ input.T\n",
    "print(attn_scores)\n",
    "\n",
    "print(f'tensor shape = {attn_scores.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n",
      "row0 = 0.9999999403953552\n",
      "row1 = 1.0\n",
      "row2 = 1.0\n",
      "row3 = 1.0\n",
      "row4 = 1.0000001192092896\n",
      "row5 = 1.0\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim = 1) # softmax over the column dimension \n",
    "print(attn_weights)\n",
    "\n",
    "# Quick verification that each row sums to 1 \n",
    "for i, x_i in enumerate(attn_weights):\n",
    "    print(f'row{i} = {sum(x_i)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n",
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# Copmute Context vector\n",
    "all_context_vecs = attn_weights @ inputs \n",
    "print(all_context_vecs)\n",
    "\n",
    "print(all_context_vecs[1,:])\n",
    "# Check with previous computed second context vector \n",
    "print(context_vec_2)\n",
    "\n",
    "assert torch.equal(all_context_vecs[1,:], context_vec_2), \"Tensor are not equal\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement Self-Attention with trainable weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "x_2 = inputs[1] # second input element \n",
    "print(x_2.shape)\n",
    "d_in = inputs.shape[1]\n",
    "print(d_in)\n",
    "\n",
    "d_out = 2  # manually set output embedding size, d = 2 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query_2 = tensor([0.4306, 1.4551])\n",
      "key_2 = tensor([0.4433, 1.1419])\n",
      "keys = tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "values = tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "# Goal compute context vector \n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "# torch.nn.Parameter is a special kind of Tensor that is used to store the parameters of a model \n",
    "# requires_grad = False means that the parameter is not trainable, meaning it's fixed and freezed\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad = False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad = False)\n",
    "\n",
    "# Compute the query, key, and value vectors \n",
    "query_2 = x_2 @ W_query \n",
    "key_2 = x_2 @ W_key \n",
    "value_2 = x_2 @ W_value \n",
    "\n",
    "print(f'query_2 = {query_2}')\n",
    "print(f'key_2 = {key_2}')\n",
    "\n",
    "\n",
    "keys = inputs @ W_key \n",
    "values = inputs @ W_value \n",
    "\n",
    "print(f'keys = {keys}')\n",
    "print(f'values = {values}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n",
      "attn_scores_2 = tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n",
      "attn_weights_2 = tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "context_vec_2 = tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "# Compute unnormalized attention scores for query 2\n",
    "\n",
    "attn_scores_22 = query_2.dot(key_2)  \n",
    "print(attn_scores_22)\n",
    "\n",
    "\n",
    "attn_scores_2 = query_2@(keys.T)\n",
    "print(f'attn_scores_2 = {attn_scores_2}')\n",
    "\n",
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2/d_k**0.5, dim = 0) # 1D tensor only has one dimension \n",
    "print(f'attn_weights_2 = {attn_weights_2}')\n",
    "\n",
    "context_vec_2 = attn_weights_2 @ values \n",
    "print(f'context_vec_2 = {context_vec_2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implementing a compact SelfAttention Class \n",
    "import torch.nn as nn \n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self,x):\n",
    "        query = x @ self.W_query\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value \n",
    "        attn_scores = query @ keys.T/(query.shape[1]**0.5)\n",
    "        attn_weights = torch.softmax(attn_scores, dim = -1)\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123) \n",
    "sa_v1 = SelfAttention_v1(d_in = 3, d_out = 2)\n",
    "print(sa_v1.forward(inputs))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        query = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        attn_scores = query @ keys.T / keys.shape[-1]**0.5\n",
    "        attn_weights = torch.softmax(attn_scores, dim = -1)\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec \n",
    "    \n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs)) # we don't use sa_v2.forward(inputs)\n",
    "# because we are using the __call__ method inside nn.Module which by default calls the forward method: when we use sa_v2() it will trigger the __call__ method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hiding Future words with Causal Attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 1., 1., 1., 1.],\n",
      "        [0., 0., 1., 1., 1., 1.],\n",
      "        [0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = sa_v2.W_query(inputs) @ sa_v2.W_key(inputs).T\n",
    "\n",
    "context_length = attn_scores.shape[0]\n",
    "mask = torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
    "print(mask)\n",
    "\n",
    "attn_scores_causal = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "print(attn_scores_causal)\n",
    "\n",
    "attn_weights_causal = torch.softmax(attn_scores_causal / (keys.shape[-1]**0.5), dim = -1)\n",
    "print(attn_weights_causal)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n",
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.0000, 0.4638, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.3968, 0.3775, 0.3941, 0.0000],\n",
      "        [0.3869, 0.3327, 0.0000, 0.0000, 0.3331, 0.3058]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Apply Dropouts \n",
    "torch.manual_seed(123)\n",
    "\n",
    "dropout = torch.nn.Dropout(p = 0.5)\n",
    "example = torch.ones(6,6)\n",
    "print(dropout(example))\n",
    "\n",
    "\n",
    "print(dropout(attn_weights_causal))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nOriginal keys tensor:\\n\\nShape: [batch_size, sequence_length, d_k]\\nExample with batch_size=2, sequence_length=3, d_k=4:\\n\\nkeys = [\\n    # Batch item 1\\n    [\\n        [k11, k12, k13, k14],  # Key vector for token 1\\n        [k21, k22, k23, k24],  # Key vector for token 2\\n        [k31, k32, k33, k34]   # Key vector for token 3\\n    ],\\n    \\n    # Batch item 2\\n    [\\n        [m11, m12, m13, m14],  # Key vector for token 1\\n        [m21, m22, m23, m24],  # Key vector for token 2\\n        [m31, m32, m33, m34]   # Key vector for token 3\\n    ]\\n]\\n\\n\\nAfter keys.transpose(1,2):\\n\\nShape: [batch_size, d_k, sequence_length]\\nTransformed tensor:\\n\\ntransposed_keys = [\\n    # Batch item 1\\n    [\\n        [k11, k21, k31],  # First dimension of all key vectors\\n        [k12, k22, k32],  # Second dimension of all key vectors\\n        [k13, k23, k33],  # Third dimension of all key vectors\\n        [k14, k24, k34]   # Fourth dimension of all key vectors\\n    ],\\n    \\n    # Batch item 2\\n    [\\n        [m11, m21, m31],  # First dimension of all key vectors\\n        [m12, m22, m32],  # Second dimension of all key vectors\\n        [m13, m23, m33],  # Third dimension of all key vectors\\n        [m14, m24, m34]   # Fourth dimension of all key vectors\\n    ]\\n]\\n\\n'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Review masked_fill_() method to understand the code\n",
    "\n",
    "\n",
    "\"\"\"  \n",
    "Basic Syntax and Function\n",
    "pythontensor.masked_fill_(mask, value)\n",
    "This method:\n",
    "\n",
    "Modifies tensor in-place (that's what the underscore _ indicates)\n",
    "Replaces elements with value wherever the corresponding position in mask is True\n",
    "Leaves elements unchanged wherever mask is False\n",
    "\"\"\"\n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout = 0.5, qkv_bias = False): # context_length is the maximum sequence length the model supports\n",
    "        super().__init__()\n",
    "        self.d_out = d_out \n",
    "        self.W_query = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias = qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # register_buffer() is used for tensors that need to be asved with the model but aren't trainable parameters. \n",
    "        # Computing the causal mask repeatedly every forward pass would be wasteful, so by registering it as a buffer it's created only once during initialization and unlike weights that need to be trained, the mask is constant. \n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal = 1)) \n",
    "        # now a tensor attribute named self.mask is added to the class instance\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape \n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1,2) # Explanations below\n",
    "        # Because self.mask has a fixed shape of [context_length, context_length], but attn_scores has shape [batch_size, num_tokens, num_tokens],\n",
    "        # we want to slice the full mask to match the current tensor being masked (here is the attn_scores)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context_vec = attn_weights @ values \n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in = d_in, d_out= d_out, context_length = context_length, dropout=0.0)\n",
    "context_vec = ca(batch) \n",
    "print(context_vec)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Original keys tensor:\n",
    "\n",
    "Shape: [batch_size, sequence_length, d_k]\n",
    "Example with batch_size=2, sequence_length=3, d_k=4:\n",
    "\n",
    "keys = [\n",
    "    # Batch item 1\n",
    "    [\n",
    "        [k11, k12, k13, k14],  # Key vector for token 1\n",
    "        [k21, k22, k23, k24],  # Key vector for token 2\n",
    "        [k31, k32, k33, k34]   # Key vector for token 3\n",
    "    ],\n",
    "    \n",
    "    # Batch item 2\n",
    "    [\n",
    "        [m11, m12, m13, m14],  # Key vector for token 1\n",
    "        [m21, m22, m23, m24],  # Key vector for token 2\n",
    "        [m31, m32, m33, m34]   # Key vector for token 3\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "After keys.transpose(1,2):\n",
    "\n",
    "Shape: [batch_size, d_k, sequence_length]\n",
    "Transformed tensor:\n",
    "\n",
    "transposed_keys = [\n",
    "    # Batch item 1\n",
    "    [\n",
    "        [k11, k21, k31],  # First dimension of all key vectors\n",
    "        [k12, k22, k32],  # Second dimension of all key vectors\n",
    "        [k13, k23, k33],  # Third dimension of all key vectors\n",
    "        [k14, k24, k34]   # Fourth dimension of all key vectors\n",
    "    ],\n",
    "    \n",
    "    # Batch item 2\n",
    "    [\n",
    "        [m11, m21, m31],  # First dimension of all key vectors\n",
    "        [m12, m22, m32],  # Second dimension of all key vectors\n",
    "        [m13, m23, m33],  # Third dimension of all key vectors\n",
    "        [m14, m24, m34]   # Fourth dimension of all key vectors\n",
    "    ]\n",
    "]\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor Concatenation Along Different Dimensions Visually Explained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" \n",
    "Tensor Concatenation Along Different Dimensions Visually Explained\n",
    "\n",
    "Let's visualize concatenation with a simple example. Assume:\n",
    "\n",
    "2 attention heads\n",
    "Batch size = 2\n",
    "Sequence length = 3\n",
    "Input dimension (d_in) = 4\n",
    "Output dimension per head (d_out) = 2\n",
    "\n",
    "\n",
    "Input tensor: [2, 3, 4] (batch_size, sequence_length, d_in)\n",
    "\n",
    "[\n",
    "  # Batch item 1\n",
    "  [\n",
    "    [i111, i112, i113, i114],  # Token 1 embedding\n",
    "    [i121, i122, i123, i124],  # Token 2 embedding\n",
    "    [i131, i132, i133, i134]   # Token 3 embedding\n",
    "  ],\n",
    "  \n",
    "  # Batch item 2\n",
    "  [\n",
    "    [i211, i212, i213, i214],  # Token 1 embedding\n",
    "    [i221, i222, i223, i224],  # Token 2 embedding\n",
    "    [i231, i232, i233, i234]   # Token 3 embedding\n",
    "  ]\n",
    "]\n",
    "\n",
    "\n",
    "Each head outputs: [2, 3, 2] (batch_size, sequence_length, d_out)\n",
    "# Output from Head 1\n",
    "[\n",
    "  # Batch item 1\n",
    "  [\n",
    "    [h1_111, h1_112],  # Token 1 output\n",
    "    [h1_121, h1_122],  # Token 2 output\n",
    "    [h1_131, h1_132]   # Token 3 output\n",
    "  ],\n",
    "  \n",
    "  # Batch item 2\n",
    "  [\n",
    "    [h1_211, h1_212],  # Token 1 output\n",
    "    [h1_221, h1_222],  # Token 2 output\n",
    "    [h1_231, h1_232]   # Token 3 output\n",
    "  ]\n",
    "]\n",
    "\n",
    "# Output from Head 2\n",
    "[\n",
    "  # Batch item 1\n",
    "  [\n",
    "    [h2_111, h2_112],  # Token 1 output\n",
    "    [h2_121, h2_122],  # Token 2 output\n",
    "    [h2_131, h2_132]   # Token 3 output\n",
    "  ],\n",
    "  \n",
    "  # Batch item 2\n",
    "  [\n",
    "    [h2_211, h2_212],  # Token 1 output\n",
    "    [h2_221, h2_222],  # Token 2 output\n",
    "    [h2_231, h2_232]   # Token 3 output\n",
    "  ]\n",
    "]\n",
    "\n",
    "\n",
    "Concatenation along different dimensions:\n",
    "1. Along dimension -1 (or 2) - Feature dimension (what's used in code):\n",
    "Output shape: [2, 3, 4] (batch_size, sequence_length, d_out*num_heads)\n",
    "\n",
    "[\n",
    "  # Batch item 1\n",
    "  [\n",
    "    [h1_111, h1_112, h2_111, h2_112],  # Token 1 features from both heads\n",
    "    [h1_121, h1_122, h2_121, h2_122],  # Token 2 features from both heads\n",
    "    [h1_131, h1_132, h2_131, h2_132]   # Token 3 features from both heads\n",
    "  ],\n",
    "  \n",
    "  # Batch item 2\n",
    "  [\n",
    "    [h1_211, h1_212, h2_211, h2_212],  # Token 1 features from both heads\n",
    "    [h1_221, h1_222, h2_221, h2_222],  # Token 2 features from both heads\n",
    "    [h1_231, h1_232, h2_231, h2_232]   # Token 3 features from both heads\n",
    "  ]\n",
    "]\n",
    "\n",
    "2. Along dimension 0 - Batch dimension:\n",
    "Output shape: [4, 3, 2] (batch_size*num_heads, sequence_length, d_out)\n",
    "\n",
    "[\n",
    "  # Batch 1, Head 1\n",
    "  [\n",
    "    [h1_111, h1_112],\n",
    "    [h1_121, h1_122],\n",
    "    [h1_131, h1_132]\n",
    "  ],\n",
    "  \n",
    "  # Batch 2, Head 1\n",
    "  [\n",
    "    [h1_211, h1_212],\n",
    "    [h1_221, h1_222],\n",
    "    [h1_231, h1_232]\n",
    "  ],\n",
    "  \n",
    "  # Batch 1, Head 2\n",
    "  [\n",
    "    [h2_111, h2_112],\n",
    "    [h2_121, h2_122],\n",
    "    [h2_131, h2_132]\n",
    "  ],\n",
    "  \n",
    "  # Batch 2, Head 2\n",
    "  [\n",
    "    [h2_211, h2_212],\n",
    "    [h2_221, h2_222],\n",
    "    [h2_231, h2_232]\n",
    "  ]\n",
    "]\n",
    "\n",
    "3. Along dimension 1 - Sequence dimension:\n",
    "Output shape: [2, 6, 2] (batch_size, sequence_length*num_heads, d_out)\n",
    "\n",
    "[\n",
    "  # Batch item 1\n",
    "  [\n",
    "    [h1_111, h1_112],  # Token 1, Head 1\n",
    "    [h1_121, h1_122],  # Token 2, Head 1\n",
    "    [h1_131, h1_132],  # Token 3, Head 1\n",
    "    [h2_111, h2_112],  # Token 1, Head 2\n",
    "    [h2_121, h2_122],  # Token 2, Head 2\n",
    "    [h2_131, h2_132]   # Token 3, Head 2\n",
    "  ],\n",
    "  \n",
    "  # Batch item 2\n",
    "  [\n",
    "    [h1_211, h1_212],  # Token 1, Head 1\n",
    "    [h1_221, h1_222],  # Token 2, Head 1\n",
    "    [h1_231, h1_232],  # Token 3, Head 1\n",
    "    [h2_211, h2_212],  # Token 1, Head 2\n",
    "    [h2_221, h2_222],  # Token 2, Head 2\n",
    "    [h2_231, h2_232]   # Token 3, Head 2\n",
    "  ]\n",
    "]\n",
    "\n",
    "\n",
    "维度2（特征维度）拼接的优势\n",
    "保持数据结构的语义完整性\n",
    "\n",
    "批次大小（batch_size）保持不变：每个样本的独立性得到保持\n",
    "序列长度（sequence_length）保持不变：每个token的位置信息得到保持\n",
    "只是增加了每个token的特征维度：将不同注意力头的特征信息组合在一起\n",
    "直观的特征表示\n",
    "\n",
    "对于每个token，我们得到的是所有注意力头的特征的组合\n",
    "相当于每个token都获得了来自不同注意力头的\"视角\"或\"表示\"\n",
    "\n",
    "Preserves Data Structural Semantics\n",
    "\n",
    "Batch size remains unchanged: maintains sample independence\n",
    "Sequence length remains unchanged: preserves token position information\n",
    "Only increases feature dimension: combines information from different attention heads\n",
    "Intuitive Feature Representation\n",
    "\n",
    "For each token, we get a combination of features from all attention heads\n",
    "Each token receives \"perspectives\" or \"representations\" from different attention heads\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "            for _ in range(num_heads)]\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        context_vecs = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        return context_vecs\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # number of tokens \n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.00, num_heads = 2)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "MultiHeadAttentionWrapper\n",
    "└── self.heads (nn.ModuleList)\n",
    "    ├── self.heads[0] (CausalAttention)\n",
    "    │   ├── self.W_query (nn.Linear)\n",
    "    │   │   ├── weight (nn.Parameter)\n",
    "    │   │   └── bias (nn.Parameter) [if qkv_bias=True]\n",
    "    │   ├── self.W_key (nn.Linear)\n",
    "    │   │   ├── weight (nn.Parameter)\n",
    "    │   │   └── bias (nn.Parameter) [if qkv_bias=True]\n",
    "    │   ├── self.W_value (nn.Linear)\n",
    "    │   │   ├── weight (nn.Parameter)\n",
    "    │   │   └── bias (nn.Parameter) [if qkv_bias=True]\n",
    "    │   ├── self.dropout (nn.Dropout)\n",
    "    │   └── self.mask (registered buffer)\n",
    "    ├── self.heads[1] (CausalAttention)\n",
    "    │   [Same structure as heads[0]]\n",
    "    └── [Additional heads...]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' \\nWithout Output Projection:\\nHead 1 output: [0.5, 0.8]\\nHead 2 output: [0.3, 0.2]\\n\\nFinal output after concatenation: [0.5, 0.8, 0.3, 0.2]\\n\\n\\n\\nWith Output Projection:\\nHead 1 output: [0.5, 0.8]\\nHead 2 output: [0.3, 0.2]\\n\\nConcatenated: [0.5, 0.8, 0.3, 0.2]\\n\\nWeights matrix example:\\nW_out = [[0.1, 0.7, 0.2, 0.0],\\n         [0.5, 0.1, 0.4, 0.3],\\n         [0.0, 0.3, 0.9, 0.2],\\n         [0.2, 0.5, 0.1, 0.8]]\\n\\nFinal output after projection:\\n[0.34, 0.57, 0.54, 0.69]\\n\\nThe gist is that output projection gives more flexible representation and the final representation can distribute information optimally.\\n\\n\\nThe final output is calculated by multiplying the concatenated vector by the weight matrix:\\nOutput[0] - First output dimension:\\n(0.5 × 0.1) + (0.8 × 0.7) + (0.3 × 0.2) + (0.2 × 0.0) = 0.05 + 0.56 + 0.06 + 0 = 0.67\\nOutput[1] - Second output dimension:\\n(0.5 × 0.5) + (0.8 × 0.1) + (0.3 × 0.4) + (0.2 × 0.3) = 0.25 + 0.08 + 0.12 + 0.06 = 0.51\\nOutput[2] - Third output dimension:\\n(0.5 × 0.0) + (0.8 × 0.3) + (0.3 × 0.9) + (0.2 × 0.2) = 0 + 0.24 + 0.27 + 0.04 = 0.55\\nOutput[3] - Fourth output dimension:\\n(0.5 × 0.2) + (0.8 × 0.5) + (0.3 × 0.1) + (0.2 × 0.8) = 0.1 + 0.4 + 0.03 + 0.16 = 0.69\\nSo the correct final output is [0.67, 0.51, 0.55, 0.69].\\nWhat \"Blending Information Across Heads\" Means\\nLooking at the calculations above, we can see this blending happening:\\nExample: Output[2] = 0.55\\n\\nReceives 0% contribution from first dimension of Head 1 (weight = 0.0)\\nReceives 24% contribution from second dimension of Head 1 (0.8 × 0.3 = 0.24)\\nReceives 49% contribution from first dimension of Head 2 (0.3 × 0.9 = 0.27)\\nReceives 7% contribution from second dimension of Head 2 (0.2 × 0.2 = 0.04)\\n\\nThis shows how the third dimension of the output (0.55) is primarily being influenced by information from Head 2\\'s first dimension and Head 1\\'s second dimension. The model is learning to extract and combine specific features from different attention heads.\\n\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out, self.num_heads = d_out, num_heads \n",
    "        self.head_dim = self.d_out // self.num_heads\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, qkv_bias)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\", \n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal = 1)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape \n",
    "        \n",
    "        queries = self.W_query(x)\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Because now queries, key, values have shape (batch_size, num_tokens, d_out)\n",
    "        # We want to split them into num_heads, each with shape (batch_size, num_tokens, num_heads, head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Now we want to make them more intuitive by having the shape (batch_size, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3)\n",
    "        \n",
    "        # Use the mask: first truncate the mask to have the same shape as attn_scores, then convert them into boolean so we can use masked_fill_() method \n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores/keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # \n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        # Now the context_vec has shape (batch_size, attn_heads, num_tokens, head_dim)\n",
    "        # We want to convert it to shape (batch_size, num_tokens, num_heads, head_dim) so we can combine the last dimensions back to d_out\n",
    "        context_vec = context_vec.transpose(1,2)\n",
    "        # Now we want to combine the last two dimension to get the final shape (batch_size, num_tokens, d_out)\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out) # we use .contiguous() to make sure the memory is contiguous because .view() requires contiguousu memory\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        return context_vec \n",
    "\n",
    "torch.manual_seed(123)\n",
    "batch_size, num_tokens, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads = 2, qkv_bias = False)\n",
    "context_vecs = mha(batch)\n",
    "print(context_vecs)\n",
    "\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" \n",
    "Without Output Projection:\n",
    "Head 1 output: [0.5, 0.8]\n",
    "Head 2 output: [0.3, 0.2]\n",
    "\n",
    "Final output after concatenation: [0.5, 0.8, 0.3, 0.2]\n",
    "\n",
    "\n",
    "\n",
    "With Output Projection:\n",
    "Head 1 output: [0.5, 0.8]\n",
    "Head 2 output: [0.3, 0.2]\n",
    "\n",
    "Concatenated: [0.5, 0.8, 0.3, 0.2]\n",
    "\n",
    "Weights matrix example:\n",
    "W_out = [[0.1, 0.7, 0.2, 0.0],\n",
    "         [0.5, 0.1, 0.4, 0.3],\n",
    "         [0.0, 0.3, 0.9, 0.2],\n",
    "         [0.2, 0.5, 0.1, 0.8]]\n",
    "\n",
    "Final output after projection:\n",
    "[0.34, 0.57, 0.54, 0.69]\n",
    "\n",
    "The gist is that output projection gives more flexible representation and the final representation can distribute information optimally.\n",
    "\n",
    "\n",
    "The final output is calculated by multiplying the concatenated vector by the weight matrix:\n",
    "Output[0] - First output dimension:\n",
    "(0.5 × 0.1) + (0.8 × 0.7) + (0.3 × 0.2) + (0.2 × 0.0) = 0.05 + 0.56 + 0.06 + 0 = 0.67\n",
    "Output[1] - Second output dimension:\n",
    "(0.5 × 0.5) + (0.8 × 0.1) + (0.3 × 0.4) + (0.2 × 0.3) = 0.25 + 0.08 + 0.12 + 0.06 = 0.51\n",
    "Output[2] - Third output dimension:\n",
    "(0.5 × 0.0) + (0.8 × 0.3) + (0.3 × 0.9) + (0.2 × 0.2) = 0 + 0.24 + 0.27 + 0.04 = 0.55\n",
    "Output[3] - Fourth output dimension:\n",
    "(0.5 × 0.2) + (0.8 × 0.5) + (0.3 × 0.1) + (0.2 × 0.8) = 0.1 + 0.4 + 0.03 + 0.16 = 0.69\n",
    "So the correct final output is [0.67, 0.51, 0.55, 0.69].\n",
    "What \"Blending Information Across Heads\" Means\n",
    "Looking at the calculations above, we can see this blending happening:\n",
    "Example: Output[2] = 0.55\n",
    "\n",
    "Receives 0% contribution from first dimension of Head 1 (weight = 0.0)\n",
    "Receives 24% contribution from second dimension of Head 1 (0.8 × 0.3 = 0.24)\n",
    "Receives 49% contribution from first dimension of Head 2 (0.3 × 0.9 = 0.27)\n",
    "Receives 7% contribution from second dimension of Head 2 (0.2 × 0.2 = 0.04)\n",
    "\n",
    "This shows how the third dimension of the output (0.55) is primarily being influenced by information from Head 2's first dimension and Head 1's second dimension. The model is learning to extract and combine specific features from different attention heads.\n",
    "\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-from-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
